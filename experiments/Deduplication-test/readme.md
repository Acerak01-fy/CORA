# 测试数据去重
## 目的
测试数据去重情况。对比DADI的固定大小分块。
### 测试数据集
为全面评估 CORA 在不同数据特征下的性能，本文设计了三类测试数据集：
1. 随机数据（Random）：使用 /dev/urandom 生成，模拟不可压缩的最坏情况。
2. 模式数据（Pattern）：通过重复固定字符串生成，模拟高度可压缩的场景。
3. 混合数据（Mixed）：交替使用 4KB 重复块和 4KB 随机块，模拟真实容器镜像的混合
场景。

每类数据集包含三种文件大小：100KB、1MB、10MB，共计 9 组测试用例。
### 测试方法与指标
• 重复次数：每组测试用例重复执行 5 次。
• 数据记录与分析：自动记录压缩后大小、耗时等数据，并计算平均值与标准差。
• 完整性验证：对每次解压缩结果进行 MD5 哈希校验，确保数据与原始数据完全一致。
• 核心性能指标：
– 空间节省率（Space Saving）：评估去重效果。
– 压缩/解压缩加速比（Speedup Ratio）：衡量处理效率。
– 数据完整性：MD5 校验通过率必须为 100%。
总计执行了 90 组基准测试（9 种配置 × 2 种方法 × 5 次重复）。
## 结果

表 2 从宏观上展示了在三种不同数据类型下，CORA 方案的综合表现。

### 表 2: FastCDC vs 固定分块综合性能对比

| 数据类型 | 平均空间节省率 | 平均解压缩加速比 | MD5 通过率 |
| :--- | :---: | :---: | :---: |
| 随机数据（Random） | -0.10% | 8.31× | 100% |
| 模式数据（Pattern） | 61.51% | 8.58× | 100% |
| 混合数据（Mixed） | 72.36% | 8.79× | 100% |

### 关键发现：

• **高去重率**：在最接近真实场景的“混合数据”中，FastCDC 实现了 **72.36%** 的空间节省，证明其在识别跨边界重复数据方面的卓越能力。

• **高稳定性**：即使在不可压缩的“随机数据”场景下，空间开销也仅为 0.10%，无性能退化。

• **数据完整性**：所有 90 组测试的 MD5 校验通过率均为 **100%**，证明方案的可靠性。
